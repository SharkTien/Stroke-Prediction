import numpy as np

class LinearSVM:
    """
    MÃ´ hÃ¬nh SVM tuyáº¿n tÃ­nh (Linear Support Vector Machine) Ä‘Æ°á»£c cÃ i Ä‘áº·t tá»« Ä‘áº§u
    báº±ng Mini-Batch Gradient Descent vÃ  sá»­ dá»¥ng hÃ m máº¥t mÃ¡t Hinge Loss.

    ğŸ¯ Má»¥c tiÃªu:
    - PhÃ¢n loáº¡i nhá»‹ phÃ¢n vá»›i SVM tuyáº¿n tÃ­nh.
    - Tá»‘i Æ°u hÃ³a hÃ m Hinge Loss báº±ng thuáº­t toÃ¡n mini-batch GD.
    - Há»— trá»£ Ä‘iá»u chuáº©n L2 Ä‘á»ƒ trÃ¡nh overfitting.

    Thuá»™c tÃ­nh:
    -----------
    - learning_rate (float): tá»‘c Ä‘á»™ há»c (alpha) trong gradient descent.
    - lambda_param (float): há»‡ sá»‘ Ä‘iá»u chuáº©n L2.
    - n_iters (int): sá»‘ vÃ²ng láº·p huáº¥n luyá»‡n toÃ n bá»™ táº­p dá»¯ liá»‡u.
    - batch_size (int): kÃ­ch thÆ°á»›c má»—i mini-batch khi huáº¥n luyá»‡n.
    - weights (np.ndarray): vector trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh (há»c Ä‘Æ°á»£c).
    - bias (float): há»‡ sá»‘ chá»‡ch (intercept).
    - loss_history (list): danh sÃ¡ch giÃ¡ trá»‹ máº¥t mÃ¡t sau má»—i vÃ²ng láº·p, Ä‘á»ƒ theo dÃµi quÃ¡ trÃ¬nh há»c.

    PhÆ°Æ¡ng thá»©c:
    -----------
    - fit(X, y): huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»« dá»¯ liá»‡u Ä‘áº§u vÃ o.
    - predict(X): dá»± Ä‘oÃ¡n nhÃ£n lá»›p (0 hoáº·c 1) tá»« dá»¯ liá»‡u Ä‘áº§u vÃ o.
    - predict_proba(X): dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cá»§a lá»›p dÆ°Æ¡ng (dáº¡ng gáº§n Ä‘Ãºng).
    """

    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=2000, batch_size=32):
        """
        Khá»Ÿi táº¡o cÃ¡c siÃªu tham sá»‘ vÃ  biáº¿n lÆ°u tráº¡ng thÃ¡i mÃ´ hÃ¬nh.
        """
        self.lr = learning_rate              # Tá»‘c Ä‘á»™ há»c
        self.lambda_param = lambda_param     # Äiá»u chuáº©n L2
        self.n_iters = n_iters               # Sá»‘ vÃ²ng láº·p (epoch)
        self.batch_size = batch_size         # Sá»‘ máº«u má»—i batch

        self.weights = None                  # Trá»ng sá»‘ mÃ´ hÃ¬nh
        self.bias = None                     # Bias (intercept)
        self.loss_history = []               # LÆ°u láº¡i loss sau má»—i epoch

    def fit(self, X, y, verbose=False):
        """
        Huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng mini-batch gradient descent.
        """
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)  # Trá»ng sá»‘ khá»Ÿi táº¡o
        self.bias = 0                        # Bias ban Ä‘áº§u

        # Chuyá»ƒn nhÃ£n tá»« 0 â†’ -1 Ä‘á»ƒ phÃ¹ há»£p vá»›i cÃ´ng thá»©c SVM
        y_ = np.where(y <= 0, -1, 1)

        for i in range(self.n_iters):
            # XÃ¡o trá»™n dá»¯ liá»‡u má»—i epoch
            indices = np.random.permutation(n_samples)
            X_shuffled = X[indices]
            y_shuffled = y_[indices]

            # Duyá»‡t tá»«ng mini-batch
            for start in range(0, n_samples, self.batch_size):
                end = start + self.batch_size
                X_batch = X_shuffled[start:end]
                y_batch = y_shuffled[start:end]

                # Kiá»ƒm tra Ä‘iá»u kiá»‡n margin: y(w.x + b) â‰¥ 1
                condition = y_batch * (np.dot(X_batch, self.weights) + self.bias) >= 1

                dw = np.zeros(n_features)  # Gradient cá»§a weights
                db = 0                    # Gradient cá»§a bias

                # Duyá»‡t tá»«ng máº«u trong batch Ä‘á»ƒ tÃ­nh gradient
                for idx, cond in enumerate(condition):
                    if cond:
                        # Náº¿u phÃ¢n loáº¡i Ä‘Ãºng vÃ  Ä‘á»§ margin, chá»‰ cáº§n regularization
                        dw += 2 * self.lambda_param * self.weights
                        db += 0
                    else:
                        # Náº¿u vi pháº¡m margin, thÃªm pháº§n Ä‘áº¡o hÃ m tá»« hinge loss
                        dw += 2 * self.lambda_param * self.weights - y_batch[idx] * X_batch[idx]
                        db += -y_batch[idx]

                # Trung bÃ¬nh gradient theo batch
                dw /= len(X_batch)
                db /= len(X_batch)

                # Cáº­p nháº­t trá»ng sá»‘
                self.weights -= self.lr * dw
                self.bias -= self.lr * db

            # TÃ­nh loss toÃ n bá»™ sau má»—i epoch (Ä‘á»ƒ theo dÃµi há»™i tá»¥)
            hinge_losses = np.maximum(0, 1 - y_ * (np.dot(X, self.weights) + self.bias))
            loss = np.mean(hinge_losses) + self.lambda_param * np.sum(self.weights ** 2)
            self.loss_history.append(loss)

            # In loss náº¿u báº­t verbose
            if verbose and i % 100 == 0:
                print(f"Iteration {i}, Loss: {loss:.4f}")

    def predict(self, X):
        """
        Dá»± Ä‘oÃ¡n nhÃ£n lá»›p: 1 náº¿u Ä‘áº§u ra â‰¥ 0, ngÆ°á»£c láº¡i lÃ  0.
        """
        linear_output = np.dot(X, self.weights) + self.bias
        return np.where(linear_output >= 0, 1, 0)

    def predict_proba(self, X):
        """
        Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t lá»›p dÆ°Æ¡ng báº±ng sigmoid (xáº¥p xá»‰ xÃ¡c suáº¥t).
        DÃ¹ khÃ´ng Ä‘Ãºng vá» báº£n cháº¥t vá»›i SVM, nhÆ°ng há»¯u Ã­ch Ä‘á»ƒ tÃ­nh AUC.
        """
        raw_scores = np.dot(X, self.weights) + self.bias
        raw_scores = np.clip(raw_scores, -10, 10)  # TrÃ¡nh overflow
        return 1 / (1 + np.exp(-raw_scores))

